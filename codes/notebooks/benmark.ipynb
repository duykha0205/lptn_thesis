{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# import time\n",
    "\n",
    "# def benchmark_activation(activation_function):\n",
    "#     start_time = time.time()\n",
    "#     # nn.Sequential(nn.Linear(10, 10), nn.Activation(activation_function)).train()\n",
    "#     nn.Sequential(nn.Linear(10, 10), activation_function).train()\n",
    "#     end_time = time.time()\n",
    "#     return end_time - start_time\n",
    "\n",
    "# activation_functions = [nn.ReLU(), nn.LeakyReLU(), nn.SiLU()]\n",
    "# training_times = [benchmark_activation(activation_function)*1000 for activation_function in activation_functions]\n",
    "\n",
    "# print(f\"Training times (in mili seconds): {training_times}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kha/.virtualenvs/kha/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training times of ReLU Activation (in milliseconds): 514.5785808563232\n",
      "\n",
      "Training times of LeakyReLU Activation (in milliseconds): 42.48785972595215\n",
      "\n",
      "Training times of SiLU Activation (in milliseconds): 38.384437561035156\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import time\n",
    "\n",
    "def benchmark_activation(activation_function, input_size):\n",
    "    model = nn.Sequential(nn.Linear(input_size, 10), activation_function)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    # Generate random tensor data\n",
    "    batch_size = 32\n",
    "    x_train = torch.randn(batch_size, input_size)\n",
    "    y_train = torch.randn(batch_size, 10)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Train the model for a few iterations\n",
    "    for _ in range(100):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    end_time = time.time()\n",
    "    return (end_time - start_time) * 1000  # Convert to milliseconds\n",
    "\n",
    "input_size = 10\n",
    "# activation_functions = [nn.ReLU(), nn.LeakyReLU(), nn.SiLU()]\n",
    "# training_times = [benchmark_activation(activation_function, input_size) for activation_function in activation_functions]\n",
    "\n",
    "# print(f\"Training times (in milliseconds): {training_times}\")\n",
    "\n",
    "activation_function = nn.ReLU()\n",
    "training_times = benchmark_activation(activation_function, input_size)\n",
    "print(f\"Training times of ReLU Activation (in milliseconds): {training_times}\")\n",
    "print()\n",
    "\n",
    "activation_function = nn.LeakyReLU()\n",
    "training_times = benchmark_activation(activation_function, input_size)\n",
    "print(f\"Training times of LeakyReLU Activation (in milliseconds): {training_times}\")\n",
    "print()\n",
    "\n",
    "activation_function = nn.SiLU()\n",
    "training_times = benchmark_activation(activation_function, input_size)\n",
    "print(f\"Training times of SiLU Activation (in milliseconds): {training_times}\")\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import time\n",
    "\n",
    "# # Define your neural network architecture\n",
    "# class Model(nn.Module):\n",
    "#     def __init__(self, activation):\n",
    "#         super(Model, self).__init__()\n",
    "#         self.fc1 = nn.Linear(1000, 64)\n",
    "#         self.fc2 = nn.Linear(64, 64)\n",
    "#         self.fc3 = nn.Linear(64, 10)\n",
    "#         self.activation = activation\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.activation(self.fc1(x))\n",
    "#         x = self.activation(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "#         return x\n",
    "\n",
    "# # Generate random tensor data\n",
    "# input_size = 1000\n",
    "# batch_size = 32\n",
    "# num_batches = 100\n",
    "# x_train = torch.randn(batch_size * num_batches, input_size)\n",
    "# y_train = torch.randint(5, 10, (batch_size * num_batches,))\n",
    "\n",
    "# # Define the activation functions to benchmark\n",
    "# # activation_functions = ['relu', 'sigmoid', 'tanh']\n",
    "# activation_functions = [nn.ReLU(), nn.LeakyReLU(), nn.SiLU()]\n",
    "\n",
    "# # Benchmark training time for each activation function\n",
    "# for activation in activation_functions:\n",
    "#     model = Model(activation)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     # Train the model\n",
    "#     for epoch in range(5):\n",
    "#         for batch in range(num_batches):\n",
    "#             batch_start = batch * batch_size\n",
    "#             batch_end = (batch + 1) * batch_size\n",
    "#             inputs = x_train[batch_start:batch_end]\n",
    "#             labels = y_train[batch_start:batch_end]\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#     end_time = time.time()\n",
    "#     training_time = end_time - start_time\n",
    "\n",
    "#     print(f\"Activation: {activation} | Training Time: {training_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ActNorm(nn.Module):\n",
    "    def __init__(self, num_features, logdet=False, affine=True,\n",
    "                 allow_reverse_init=False):\n",
    "        assert affine\n",
    "        super().__init__()\n",
    "        self.logdet = logdet\n",
    "        self.loc = nn.Parameter(torch.zeros(1, num_features, 1, 1))\n",
    "        self.scale = nn.Parameter(torch.ones(1, num_features, 1, 1))\n",
    "        self.allow_reverse_init = allow_reverse_init\n",
    "\n",
    "        self.register_buffer('initialized', torch.tensor(0, dtype=torch.uint8))\n",
    "\n",
    "    def initialize(self, input):\n",
    "        with torch.no_grad():\n",
    "            flatten = input.permute(1, 0, 2, 3).contiguous().view(input.shape[1], -1)\n",
    "            mean = (\n",
    "                flatten.mean(1)\n",
    "                .unsqueeze(1)\n",
    "                .unsqueeze(2)\n",
    "                .unsqueeze(3)\n",
    "                .permute(1, 0, 2, 3)\n",
    "            )\n",
    "            std = (\n",
    "                flatten.std(1)\n",
    "                .unsqueeze(1)\n",
    "                .unsqueeze(2)\n",
    "                .unsqueeze(3)\n",
    "                .permute(1, 0, 2, 3)\n",
    "            )\n",
    "\n",
    "            self.loc.data.copy_(-mean)\n",
    "            self.scale.data.copy_(1 / (std + 1e-6))\n",
    "\n",
    "    def forward(self, input, reverse=False):\n",
    "        if reverse:\n",
    "            return self.reverse(input)\n",
    "        if len(input.shape) == 2:\n",
    "            input = input[:,:,None,None]\n",
    "            squeeze = True\n",
    "        else:\n",
    "            squeeze = False\n",
    "\n",
    "        _, _, height, width = input.shape\n",
    "\n",
    "        if self.training and self.initialized.item() == 0:\n",
    "            self.initialize(input)\n",
    "            self.initialized.fill_(1)\n",
    "\n",
    "        h = self.scale * (input + self.loc)\n",
    "\n",
    "        if squeeze:\n",
    "            h = h.squeeze(-1).squeeze(-1)\n",
    "\n",
    "        if self.logdet:\n",
    "            log_abs = torch.log(torch.abs(self.scale))\n",
    "            logdet = height*width*torch.sum(log_abs)\n",
    "            logdet = logdet * torch.ones(input.shape[0]).to(input)\n",
    "            return h, logdet\n",
    "\n",
    "        return h\n",
    "\n",
    "    def reverse(self, output):\n",
    "        if self.training and self.initialized.item() == 0:\n",
    "            if not self.allow_reverse_init:\n",
    "                raise RuntimeError(\n",
    "                    \"Initializing ActNorm in reverse direction is \"\n",
    "                    \"disabled by default. Use allow_reverse_init=True to enable.\"\n",
    "                )\n",
    "            else:\n",
    "                self.initialize(output)\n",
    "                self.initialized.fill_(1)\n",
    "\n",
    "        if len(output.shape) == 2:\n",
    "            output = output[:,:,None,None]\n",
    "            squeeze = True\n",
    "        else:\n",
    "            squeeze = False\n",
    "\n",
    "        h = output / self.scale - self.loc\n",
    "\n",
    "        if squeeze:\n",
    "            h = h.squeeze(-1).squeeze(-1)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training times ActNorm (in mili seconds): 72.14639186859131\n",
      "Training times InstanceNorm2d (in mili seconds): 52.314162254333496\n",
      "Training times BatchNorm2d (in mili seconds): 63.10415267944336\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def benchmark_activation(activation_function, input_size):\n",
    "    model = nn.Sequential(nn.Linear(input_size, 10), activation_function)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    # Generate random tensor data\n",
    "    batch_size = 32\n",
    "    x_train = torch.randn(batch_size, input_size)\n",
    "    y_train = torch.randn(batch_size, 10)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Train the model for a few iterations\n",
    "    for _ in range(100):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    end_time = time.time()\n",
    "    return (end_time - start_time) * 1000  # Convert to milliseconds\n",
    "\n",
    "input_size = 10\n",
    "\n",
    "activation_functions = [ActNorm(input_size)]*10\n",
    "training_times = [benchmark_activation(activation_function, input_size) for activation_function in activation_functions]\n",
    "print(f\"Training times ActNorm (in mili seconds): {np.mean(training_times)}\")\n",
    "\n",
    "activation_functions = [nn.InstanceNorm1d(input_size)]*10\n",
    "training_times = [benchmark_activation(activation_function, input_size) for activation_function in activation_functions]\n",
    "print(f\"Training times InstanceNorm2d (in mili seconds): {np.mean(training_times)}\")\n",
    "\n",
    "activation_functions = [nn.BatchNorm1d(input_size)]*10\n",
    "training_times = [benchmark_activation(activation_function, input_size) for activation_function in activation_functions]\n",
    "print(f\"Training times BatchNorm2d (in mili seconds): {np.mean(training_times)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_divisible(v, divisor=8, min_value=None, round_limit=.9):\n",
    "    min_value = min_value or divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < round_limit * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "class SEModule(nn.Module):\n",
    "    \"\"\" SE Module as defined in original SE-Nets with a few additions\n",
    "    Additions include:\n",
    "        * divisor can be specified to keep channels % div == 0 (default: 8)\n",
    "        * reduction channels can be specified directly by arg (if rd_channels is set)\n",
    "        * reduction channels can be specified by float rd_ratio (default: 1/16)\n",
    "        * global max pooling can be added to the squeeze aggregation\n",
    "        * customizable activation, normalization, and gate layer\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, channels, rd_ratio=1. / 16, rd_channels=None, rd_divisor=8, add_maxpool=True,\n",
    "            bias=True, act_layer=nn.ReLU, norm_layer=ActNorm, gate_layer='sigmoid'):\n",
    "        super(SEModule, self).__init__()\n",
    "        self.add_maxpool = add_maxpool\n",
    "        if not rd_channels:\n",
    "            rd_channels = make_divisible(channels * rd_ratio, rd_divisor, round_limit=0.)\n",
    "        self.fc1 = nn.Conv2d(channels, rd_channels, kernel_size=1, bias=bias)\n",
    "        self.bn = norm_layer(rd_channels) if norm_layer else nn.Identity()\n",
    "        self.act = nn.SiLU()\n",
    "        self.fc2 = nn.Conv2d(rd_channels, channels, kernel_size=1, bias=bias)\n",
    "        self.gate = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_se = x.mean((2, 3), keepdim=True)\n",
    "        if self.add_maxpool:\n",
    "            # experimental codepath, may remove or change\n",
    "            x_se = 0.5 * x_se + 0.5 * x.amax((2, 3), keepdim=True)\n",
    "        x_se = self.fc1(x_se)\n",
    "        x_se = self.act(self.bn(x_se))\n",
    "        x_se = self.fc2(x_se)\n",
    "        return x * self.gate(x_se)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_features, in_features, 3, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(in_features, in_features, 3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training times ResidualBlock (in mili seconds): 82454.54573631287\n",
      "\n",
      "Training times SEModule (in mili seconds): 17923.808336257935\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def benchmark_residual_network(activation_function, input_size):\n",
    "    model = nn.Sequential(activation_function, activation_function, activation_function)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    # Generate random tensor data\n",
    "    batch_size = 32\n",
    "    x_train = torch.randn(batch_size, input_size, input_size, input_size)\n",
    "    # x_train = torch.randn(batch_size, input_size)\n",
    "    y_train = torch.randn(batch_size, input_size, input_size, input_size)\n",
    "    # y_train = torch.randn(batch_size, input_size)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Train the model for a few iterations\n",
    "    for _ in range(100):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    end_time = time.time()\n",
    "    return (end_time - start_time) * 1000  # Convert to milliseconds\n",
    "\n",
    "input_size = 10\n",
    "\n",
    "training_times = benchmark_residual_network(ResidualBlock(input_size), input_size)\n",
    "print(f\"Training times ResidualBlock (in mili seconds): {(training_times)}\")\n",
    "print()\n",
    "\n",
    "training_times = benchmark_residual_network(SEModule(input_size), input_size)\n",
    "print(f\"Training times SEModule (in mili seconds): {(training_times)}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training times old components (in mili seconds): 772.8569507598877\n",
      "\n",
      "Training times new components (in mili seconds): 351.3188362121582\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def benchmark_all_component(CNN_block, activation_function, norm_layer, input_size):\n",
    "    model = nn.Sequential( \\\n",
    "                        CNN_block,\\\n",
    "                        activation_function, \\\n",
    "                        norm_layer, \\\n",
    "                        CNN_block,\\\n",
    "                        activation_function, \\\n",
    "                        norm_layer\n",
    "                            )\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    # Generate random tensor data\n",
    "    batch_size = 32\n",
    "    x_train = torch.randn(batch_size, input_size, input_size, input_size)\n",
    "    # x_train = torch.randn(batch_size, input_size)\n",
    "    y_train = torch.randn(batch_size, input_size, input_size, input_size)\n",
    "    # y_train = torch.randn(batch_size, input_size)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Train the model for a few iterations\n",
    "    for _ in range(100):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    end_time = time.time()\n",
    "    return (end_time - start_time) * 1000  # Convert to milliseconds\n",
    "\n",
    "input_size = 10\n",
    "\n",
    "training_times = benchmark_all_component(ResidualBlock(input_size), \\\n",
    "                                            nn.LeakyReLU(), \\\n",
    "                                            nn.BatchNorm2d(input_size), \\\n",
    "                                            input_size)\n",
    "print(f\"Training times old components (in milliseconds): {(training_times)}\")\n",
    "print()\n",
    "\n",
    "training_times = benchmark_all_component(SEModule(input_size), \\\n",
    "                                            nn.SiLU(), \\\n",
    "                                            ActNorm(input_size), \\\n",
    "                                            input_size)\n",
    "print(f\"Training times new components (in milliseconds): {(training_times)}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from utils.options import dict2str, parse\n",
    "from utils.dist_util import get_dist_info, init_dist\n",
    "import random\n",
    "from utils.misc import set_random_seed\n",
    "\n",
    "\n",
    "def parse_options(is_train=True):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '-opt', type=str, default='test_speed_FiveK.yml', help='Path to option YAML file.')\n",
    "    parser.add_argument(\n",
    "        '--launcher',\n",
    "        choices=['none', 'pytorch', 'slurm'],\n",
    "        default='none',\n",
    "        help='job launcher')\n",
    "    parser.add_argument('--local_rank', type=int, default=0)\n",
    "    args = parser.parse_args()\n",
    "    opt = parse(args.opt, is_train=is_train)\n",
    "\n",
    "    # distributed settings\n",
    "    if args.launcher == 'none':\n",
    "        opt['dist'] = False\n",
    "        print('Disable distributed.', flush=True)\n",
    "    else:\n",
    "        opt['dist'] = True\n",
    "        if args.launcher == 'slurm' and 'dist_params' in opt:\n",
    "            init_dist(args.launcher, **opt['dist_params'])\n",
    "        else:\n",
    "            init_dist(args.launcher)\n",
    "\n",
    "    opt['rank'], opt['world_size'] = get_dist_info()\n",
    "\n",
    "    # random seed\n",
    "    seed = opt.get('manual_seed')\n",
    "    if seed is None:\n",
    "        seed = random.randint(1, 10000)\n",
    "        opt['manual_seed'] = seed\n",
    "    set_random_seed(seed + opt['rank'])\n",
    "\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # parse options, set distributed setting, set ramdom seed\n",
    "    opt = parse_options(is_train=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kha",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
